{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d5e52f38",
   "metadata": {},
   "source": [
    "# Uplift Modeling, Multi-Armed Bandits, and Reinforcement Learning for Personalization\n",
    "\n",
    "This notebook provides a research-level introduction to **uplift modeling**, **multi-armed bandits**, and **reinforcement learning** with **mathematical details**.  \n",
    "Applications include **fantasy sports, sports betting, mobile gaming, and consumer entertainment**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4fdd3ae",
   "metadata": {},
   "source": [
    "# 1. Uplift Modeling (Causal ML)\n",
    "\n",
    "We want to model **heterogeneous treatment effects (HTE)**:\n",
    "\n",
    "- Features: $X \\in \\mathbb{R}^d$\n",
    "- Treatment: $T \\in \\{0,1\\}$\n",
    "- Outcome: $Y$\n",
    "\n",
    "Potential outcomes:\n",
    "$$\n",
    "Y(1), Y(0), \\quad \\tau(x) = \\mathbb{E}[Y(1)-Y(0)\\mid X=x]\n",
    "$$\n",
    "\n",
    "We observe only:\n",
    "$$\n",
    "Y = T Y(1) + (1-T) Y(0)\n",
    "$$\n",
    "\n",
    "### Estimation Frameworks\n",
    "\n",
    "**T-Learner:**\n",
    "$$\n",
    "\\hat{\\mu}_1(x) = \\hat{\\mathbb{E}}[Y\\mid X=x,T=1], \\quad \n",
    "\\hat{\\mu}_0(x) = \\hat{\\mathbb{E}}[Y\\mid X=x,T=0]\n",
    "$$\n",
    "$$\n",
    "\\hat{\\tau}(x) = \\hat{\\mu}_1(x) - \\hat{\\mu}_0(x)\n",
    "$$\n",
    "\n",
    "**S-Learner:**\n",
    "$$\n",
    "\\hat{\\mu}(x,t) = \\hat{\\mathbb{E}}[Y\\mid X=x,T=t]\n",
    "$$\n",
    "$$\n",
    "\\hat{\\tau}(x) = \\hat{\\mu}(x,1)-\\hat{\\mu}(x,0)\n",
    "$$\n",
    "\n",
    "### Decision Rule\n",
    "Treat if:\n",
    "$$\n",
    "\\tau(x) V > c\n",
    "$$\n",
    "where $V$ = value per conversion, $c$ = treatment cost.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "996dcba9",
   "metadata": {},
   "source": [
    "# 2. Multi-Armed Bandits (MABs)\n",
    "\n",
    "We have $K$ arms with unknown means $\\mu_a$.  \n",
    "At each round $t$, choose arm $A_t$ and observe reward $R_t$.\n",
    "\n",
    "**Regret:**\n",
    "$$\n",
    "R(T) = T \\mu^* - \\sum_{t=1}^T \\mu_{A_t}, \\quad \\mu^* = \\max_a \\mu_a\n",
    "$$\n",
    "\n",
    "### Algorithms\n",
    "\n",
    "**Îµ-Greedy:**\n",
    "$$\n",
    "A_t = \\begin{cases}\n",
    "\\arg\\max_a \\hat{\\mu}_a & \\text{w.p. } 1-\\epsilon \\\\\n",
    "\\text{random arm} & \\text{w.p. } \\epsilon\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "**UCB1:**\n",
    "$$\n",
    "A_t = \\arg\\max_a \\left( \\hat{\\mu}_a + \\sqrt{\\tfrac{2\\ln t}{n_a}} \\right)\n",
    "$$\n",
    "\n",
    "**Thompson Sampling:**  \n",
    "For Bernoulli rewards, posterior $\\theta_a \\sim \\text{Beta}(\\alpha_a,\\beta_a)$:\n",
    "- Sample $\\tilde{\\theta}_a$ from posterior.\n",
    "- Play $A_t = \\arg\\max_a \\tilde{\\theta}_a$.\n",
    "\n",
    "### Contextual Bandits\n",
    "Rewards depend on features $X_t$:\n",
    "$$\n",
    "\\mathbb{E}[R_t \\mid A_t=a,X_t=x] = f(x,a)\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9ddfcd0",
   "metadata": {},
   "source": [
    "# 3. Reinforcement Learning (RL)\n",
    "\n",
    "Formulate personalization as an MDP:\n",
    "\n",
    "- State $s_t \\in S$\n",
    "- Action $a_t \\in A$\n",
    "- Transition $s_{t+1} \\sim P(\\cdot|s_t,a_t)$\n",
    "- Reward $r_t = R(s_t,a_t)$\n",
    "\n",
    "Objective:\n",
    "$$\n",
    "J(\\pi) = \\mathbb{E}_\\pi \\left[ \\sum_{t=0}^\\infty \\gamma^t r_t \\right]\n",
    "$$\n",
    "\n",
    "### Value Functions\n",
    "\n",
    "State-value:\n",
    "$$\n",
    "V^\\pi(s) = \\mathbb{E}_\\pi\\left[ \\sum_{t=0}^\\infty \\gamma^t r_t \\mid s_0=s \\right]\n",
    "$$\n",
    "\n",
    "Action-value:\n",
    "$$\n",
    "Q^\\pi(s,a) = \\mathbb{E}_\\pi\\left[ \\sum_{t=0}^\\infty \\gamma^t r_t \\mid s_0=s,a_0=a \\right]\n",
    "$$\n",
    "\n",
    "Bellman equations:\n",
    "$$\n",
    "V^\\pi(s) = \\sum_a \\pi(a|s) \\Big( R(s,a) + \\gamma \\sum_{s'} P(s'|s,a) V^\\pi(s') \\Big)\n",
    "$$\n",
    "\n",
    "### Learning Algorithms\n",
    "\n",
    "**Q-learning:**\n",
    "$$\n",
    "Q(s_t,a_t) \\leftarrow Q(s_t,a_t) + \\alpha [r_t + \\gamma \\max_{a'} Q(s_{t+1},a') - Q(s_t,a_t)]\n",
    "$$\n",
    "\n",
    "**Policy Gradient:**\n",
    "$$\n",
    "\\nabla_\\theta J(\\pi_\\theta) = \\mathbb{E}_{\\pi_\\theta}[ \\nabla_\\theta \\log \\pi_\\theta(a|s) Q^{\\pi_\\theta}(s,a) ]\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2af5a8ed",
   "metadata": {},
   "source": [
    "# 4. Summary Table\n",
    "\n",
    "| Method              | Objective                     | Key Equation(s)                                  | Typical Use |\n",
    "|---------------------|-------------------------------|--------------------------------------------------|-------------|\n",
    "| **Uplift Modeling** | Estimate causal effect        | $\\tau(x)=\\mathbb{E}[Y(1)-Y(0)\\mid X=x]$        | Targeting |\n",
    "| **Bandits**         | Minimize cumulative regret    | $R(T)=T\\mu^* - \\sum_{t=1}^T \\mu_{A_t}$        | Online personalization |\n",
    "| **Reinforcement Learning** | Maximize discounted reward | $J(\\pi)=\\mathbb{E}[\\sum_t \\gamma^t r_t]$ | Sequential personalization |\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}